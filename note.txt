7000:
training_args = Seq2SeqTrainingArguments(
    output_dir=model_name,       # output directory
    eval_strategy="epoch",                # evaluation strategy
    learning_rate=5e-5,                   # increased learning rate
    per_device_train_batch_size=8,        # increased batch size for training
    per_device_eval_batch_size=8,         # increased batch size for evaluation
    num_train_epochs=5,                   # reduced number of epochs
    weight_decay=0.01,                    # weight decay for regularization
    logging_dir="./logs",                 # directory for logs
    logging_steps=500,                    # log every 500 steps
    predict_with_generate=True,           # enables generate for predictions
    save_strategy="epoch",                # save model every epoch
)

500v1:
training_args = Seq2SeqTrainingArguments(
    output_dir=model_dir,       # output directory
    eval_strategy="epoch",                # evaluation strategy
    learning_rate=1e-5,                   # increased learning rate
    per_device_train_batch_size=4,        # increased batch size for training
    per_device_eval_batch_size=4,         # increased batch size for evaluation
    num_train_epochs=5,                   # reduced number of epochs
    weight_decay=0.01,                    # weight decay for regularization
    logging_dir="./logs",                 # directory for logs
    logging_steps=100,                    # log every 100 steps
    predict_with_generate=True,           # enables generate for predictions
    save_strategy="epoch",                # save model every epoch
    report_to=["wandb"],                    # enable wandb reports
)

500v2:
training_args = Seq2SeqTrainingArguments(
    output_dir=model_dir,       # output directory
    eval_strategy="epoch",                # evaluation strategy
    learning_rate=2e-5,                   # increased learning rate
    per_device_train_batch_size=4,        # increased batch size for training
    per_device_eval_batch_size=4,         # increased batch size for evaluation
    num_train_epochs=5,                   # reduced number of epochs
    weight_decay=0.01,                    # weight decay for regularization
    logging_dir="./logs",                 # directory for logs
    logging_steps=100,                    # log every 100 steps
    predict_with_generate=True,           # enables generate for predictions
    save_strategy="epoch",                # save model every epoch
    report_to=["wandb"],                    # enable wandb reports
)

500v3:
training_args = Seq2SeqTrainingArguments(
    output_dir=model_dir,       # output directory
    eval_strategy="epoch",                # evaluation strategy
    learning_rate=3e-5,                   # increased learning rate
    per_device_train_batch_size=4,        # increased batch size for training
    per_device_eval_batch_size=4,         # increased batch size for evaluation
    num_train_epochs=5,                   # reduced number of epochs
    weight_decay=0.01,                    # weight decay for regularization
    logging_dir="./logs",                 # directory for logs
    logging_steps=100,                    # log every 100 steps
    predict_with_generate=True,           # enables generate for predictions
    save_strategy="epoch",                # save model every epoch
    report_to=["wandb"],                    # enable wandb reports
)

500v4:
training_args = Seq2SeqTrainingArguments(
    output_dir=model_dir,       # output directory
    eval_strategy="epoch",                # evaluation strategy
    learning_rate=4e-5,                   # increased learning rate
    per_device_train_batch_size=4,        # increased batch size for training
    per_device_eval_batch_size=4,         # increased batch size for evaluation
    num_train_epochs=5,                   # reduced number of epochs
    weight_decay=0.01,                    # weight decay for regularization
    logging_dir="./logs",                 # directory for logs
    logging_steps=100,                    # log every 100 steps
    predict_with_generate=True,           # enables generate for predictions
    save_strategy="epoch",                # save model every epoch
    report_to=["wandb"],                    # enable wandb reports
)

500v5:
training_args = Seq2SeqTrainingArguments(
    output_dir=model_dir,       # output directory
    eval_strategy="epoch",                # evaluation strategy
    learning_rate=5e-5,                   # increased learning rate
    per_device_train_batch_size=4,        # increased batch size for training
    per_device_eval_batch_size=4,         # increased batch size for evaluation
    num_train_epochs=5,                   # reduced number of epochs
    weight_decay=0.01,                    # weight decay for regularization
    logging_dir="./logs",                 # directory for logs
    logging_steps=100,                    # log every 100 steps
    predict_with_generate=True,           # enables generate for predictions
    save_strategy="epoch",                # save model every epoch
    report_to=["wandb"],                    # enable wandb reports
)

500v6:
training_args = Seq2SeqTrainingArguments(
    output_dir=model_dir,       # output directory
    eval_strategy="epoch",                # evaluation strategy
    learning_rate=1e-7,                   # increased learning rate
    per_device_train_batch_size=4,        # increased batch size for training
    per_device_eval_batch_size=4,         # increased batch size for evaluation
    num_train_epochs=5,                   # reduced number of epochs
    weight_decay=0.01,                    # weight decay for regularization
    logging_dir="./logs",                 # directory for logs
    logging_steps=100,                    # log every 100 steps
    predict_with_generate=True,           # enables generate for predictions
    save_strategy="epoch",                # save model every epoch
    report_to=["wandb"],                    # enable wandb reports
)

500v1.1:
training_args = Seq2SeqTrainingArguments(
    output_dir=model_dir,       # output directory
    eval_strategy="epoch",                # evaluation strategy
    learning_rate=1e-5,                   # increased learning rate
    per_device_train_batch_size=4,        # increased batch size for training
    per_device_eval_batch_size=4,         # increased batch size for evaluation
    num_train_epochs=5,                   # reduced number of epochs
    weight_decay=0.03,                    # weight decay for regularization
    logging_dir="./logs",                 # directory for logs
    logging_steps=100,                    # log every 100 steps
    predict_with_generate=True,           # enables generate for predictions
    save_strategy="epoch",                # save model every epoch
    report_to=["wandb"],                    # enable wandb reports
)

500v1.2:
training_args = Seq2SeqTrainingArguments(
    output_dir=model_dir,       # output directory
    eval_strategy="epoch",                # evaluation strategy
    learning_rate=1e-5,                   # increased learning rate
    per_device_train_batch_size=4,        # increased batch size for training
    per_device_eval_batch_size=4,         # increased batch size for evaluation
    num_train_epochs=5,                   # reduced number of epochs
    weight_decay=0.05,                    # weight decay for regularization
    logging_dir="./logs",                 # directory for logs
    logging_steps=100,                    # log every 100 steps
    predict_with_generate=True,           # enables generate for predictions
    save_strategy="epoch",                # save model every epoch
    report_to=["wandb"],                    # enable wandb reports
)

500v1.2.1:
training_args = Seq2SeqTrainingArguments(
    output_dir=model_dir,       # output directory
    eval_strategy="epoch",                # evaluation strategy
    learning_rate=1e-7,                   # increased learning rate
    per_device_train_batch_size=4,        # increased batch size for training
    per_device_eval_batch_size=4,         # increased batch size for evaluation
    num_train_epochs=5,                   # reduced number of epochs
    weight_decay=0.05,                    # weight decay for regularization
    logging_dir="./logs",                 # directory for logs
    logging_steps=100,                    # log every 100 steps
    predict_with_generate=True,           # enables generate for predictions
    save_strategy="epoch",                # save model every epoch
    report_to=["wandb"],                    # enable wandb reports
)

500v1.2.2:
training_args = Seq2SeqTrainingArguments(
    output_dir=model_dir,       # output directory
    eval_strategy="epoch",                # evaluation strategy
    learning_rate=1e-6,                   # increased learning rate
    per_device_train_batch_size=4,        # increased batch size for training
    per_device_eval_batch_size=4,         # increased batch size for evaluation
    num_train_epochs=5,                   # reduced number of epochs
    weight_decay=0.05,                    # weight decay for regularization
    logging_dir="./logs",                 # directory for logs
    logging_steps=100,                    # log every 100 steps
    predict_with_generate=True,           # enables generate for predictions
    save_strategy="epoch",                # save model every epoch
    report_to=["wandb"],                    # enable wandb reports
)

500v1.2.3: 2e-6

500v1.2.4: 3e-6

500v1.2.5: 4e-6

500v1.2.6: 5e-6

500v1.2.7: 6e-6

500v1.3:
training_args = Seq2SeqTrainingArguments(
    output_dir=model_dir,       # output directory
    eval_strategy="epoch",                # evaluation strategy
    learning_rate=1e-5,                   # increased learning rate
    per_device_train_batch_size=4,        # increased batch size for training
    per_device_eval_batch_size=4,         # increased batch size for evaluation
    num_train_epochs=5,                   # reduced number of epochs
    weight_decay=0.07,                    # weight decay for regularization
    logging_dir="./logs",                 # directory for logs
    logging_steps=100,                    # log every 100 steps
    predict_with_generate=True,           # enables generate for predictions
    save_strategy="epoch",                # save model every epoch
    report_to=["wandb"],                    # enable wandb reports
)


500v1.4:
training_args = Seq2SeqTrainingArguments(
    output_dir=model_dir,       # output directory
    eval_strategy="epoch",                # evaluation strategy
    learning_rate=1e-5,                   # increased learning rate
    per_device_train_batch_size=4,        # increased batch size for training
    per_device_eval_batch_size=4,         # increased batch size for evaluation
    num_train_epochs=5,                   # reduced number of epochs
    weight_decay=0.09,                    # weight decay for regularization
    logging_dir="./logs",                 # directory for logs
    logging_steps=100,                    # log every 100 steps
    predict_with_generate=True,           # enables generate for predictions
    save_strategy="epoch",                # save model every epoch
    report_to=["wandb"],                    # enable wandb reports
)

500v1.5:
training_args = Seq2SeqTrainingArguments(
    output_dir=model_dir,       # output directory
    eval_strategy="epoch",                # evaluation strategy
    learning_rate=1e-5,                   # increased learning rate
    per_device_train_batch_size=4,        # increased batch size for training
    per_device_eval_batch_size=4,         # increased batch size for evaluation
    num_train_epochs=5,                   # reduced number of epochs
    weight_decay=0.1,                    # weight decay for regularization
    logging_dir="./logs",                 # directory for logs
    logging_steps=100,                    # log every 100 steps
    predict_with_generate=True,           # enables generate for predictions
    save_strategy="epoch",                # save model every epoch
    report_to=["wandb"],                    # enable wandb reports
)

